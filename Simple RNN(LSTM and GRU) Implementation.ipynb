{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Simple RNN(LSTM and GRU) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://deeplearningcourses.com/c/deep-learning-advanced-nlp\n",
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras.backend as K\n",
    "if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
    "  from keras.layers import CuDNNLSTM as LSTM\n",
    "  from keras.layers import CuDNNGRU as GRU\n",
    "\n",
    "\n",
    "T = 8 # Sequence length\n",
    "D = 2 # Input Dimensionality\n",
    "M = 3 # Hidden Layer size \n",
    "\n",
    "\n",
    "X = np.random.randn(1, T, D)\n",
    "\n",
    "\n",
    "def lstm1():\n",
    "  input_ = Input(shape=(T, D))\n",
    "  rnn = LSTM(M, return_state=True)\n",
    "  x = rnn(input_)\n",
    "\n",
    "  model = Model(inputs=input_, outputs=x)\n",
    "  o, h, c = model.predict(X)\n",
    "  print(\"o:\", o)\n",
    "  print(\"h:\", h)\n",
    "  print(\"c:\", c)\n",
    "\n",
    "\n",
    "def lstm2():\n",
    "  input_ = Input(shape=(T, D))\n",
    "  rnn = LSTM(M, return_state=True, return_sequences=True)\n",
    "  # rnn = GRU(M, return_state=True)\n",
    "  x = rnn(input_)\n",
    "\n",
    "  model = Model(inputs=input_, outputs=x)\n",
    "  o, h, c = model.predict(X)\n",
    "  print(\"o:\", o)\n",
    "  print(\"h:\", h)\n",
    "  print(\"c:\", c)\n",
    "\n",
    "\n",
    "def gru1():\n",
    "  input_ = Input(shape=(T, D))\n",
    "  rnn = GRU(M, return_state=True)\n",
    "  x = rnn(input_)\n",
    "\n",
    "  model = Model(inputs=input_, outputs=x)\n",
    "  o, h = model.predict(X)\n",
    "  print(\"o:\", o)\n",
    "  print(\"h:\", h)\n",
    "\n",
    "\n",
    "def gru2():\n",
    "  input_ = Input(shape=(T, D))\n",
    "  rnn = GRU(M, return_state=True, return_sequences=True)\n",
    "  x = rnn(input_)\n",
    "\n",
    "  model = Model(inputs=input_, outputs=x)\n",
    "  o, h = model.predict(X)\n",
    "  print(\"o:\", o)\n",
    "  print(\"h:\", h)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"lstm1:\")\n",
    "# lstm1()\n",
    "# print(\"lstm2:\")\n",
    "# lstm2()\n",
    "# print(\"gru1:\")\n",
    "# gru1()\n",
    "# print(\"gru2:\")\n",
    "# gru2()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm1:\n",
      "o: [[ 0.3217187   0.16237344 -0.36327904]]\n",
      "h: [[ 0.3217187   0.16237344 -0.36327904]]\n",
      "c: [[ 0.675547    0.29899424 -0.7674577 ]]\n",
      "lstm2:\n",
      "o: [[[ 5.78418113e-02 -6.72095492e-02 -4.74597327e-02]\n",
      "  [ 2.80481428e-01 -1.01390183e-01 -5.11641428e-02]\n",
      "  [ 1.69834509e-01 -5.98019622e-02  3.80781894e-05]\n",
      "  [ 5.89486808e-02 -7.51858875e-02 -2.35811919e-02]\n",
      "  [-4.56871418e-03 -4.05552015e-02 -7.17352005e-03]\n",
      "  [-5.73910773e-02  7.33386800e-02  4.98028956e-02]\n",
      "  [-1.05796196e-01  9.82574821e-02  3.89654823e-02]\n",
      "  [-8.82705152e-02  4.71285172e-03 -2.36340966e-02]]]\n",
      "h: [[-0.08827052  0.00471285 -0.0236341 ]]\n",
      "c: [[-0.3288231   0.0105776  -0.03433786]]\n"
     ]
    }
   ],
   "source": [
    "print(\"lstm1:\") # By default the return_state = True # Considers only one output that is the last output\n",
    "lstm1()\n",
    "print(\"lstm2:\") # return_sequence = True # It considers every output\n",
    "lstm2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru1:\n",
      "o: [[0.48059177 0.06008199 0.5174747 ]]\n",
      "h: [[0.48059177 0.06008199 0.5174747 ]]\n",
      "gru2:\n",
      "o: [[[ 0.00753734 -0.2082217   0.43133575]\n",
      "  [-0.32994342 -0.14123373  0.59990114]\n",
      "  [-0.07048532  0.03102256  0.13565029]\n",
      "  [ 0.21792118 -0.03484058  0.16097349]\n",
      "  [ 0.37275943  0.01243955 -0.04795794]\n",
      "  [ 0.44832247  0.06870154 -0.2834326 ]\n",
      "  [ 0.5122981   0.07244074 -0.38289058]\n",
      "  [ 0.6593656  -0.20818871  0.2791289 ]]]\n",
      "h: [[ 0.6593656  -0.20818871  0.2791289 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"gru1:\") # By default the return_state = True  # Considers only one output that is the last output\n",
    "gru1()\n",
    "print(\"gru2:\") # return_sequence = True # It considers every output\n",
    "gru2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
